---
slug: "local-ai"
title: "Ускоряем разработку с Ollama, RAG и интеграцией в IDE"
description: "ИИ-агенты и LLM сегодня активно помогают разработчикам ускорять работу: автоматизируют рутинные задачи, генерируют код, отвечают на вопросы по документации и даже помогают с отладкой."
summary: "ИИ-агенты и LLM сегодня активно помогают разработчикам ускорять работу: автоматизируют рутинные задачи, генерируют код, отвечают на вопросы по документации и даже помогают с отладкой."
image: "/posts/ollama.png"
date: 2025-07-27
tags: [ai ollama]
---

![Ускоряем разработку с Ollama, RAG и интеграцией в IDE](/posts/ollama.png "Ускоряем разработку с Ollama, RAG и интеграцией в IDE")

### Локальные ИИ-агенты и инструменты
ИИ-агенты и LLM сегодня активно помогают разработчикам ускорять работу: автоматизируют рутинные задачи, генерируют код, отвечают на вопросы по документации и даже помогают с отладкой. В этой статье мы рассмотрим, как быстро развернуть Ollama — локальный сервер LLM, использовать RAG для работы с собственными данными, а также интегрировать всё это в удобный интерфейс.

#### Глоссарий
- **LLM** - Большая языковая модель (deepseek-r1, gemma3, qwen3, mistral)
- **RAG** - Retrieval-Augmented Generation. Полноценная архитектура с векторными эмбеддингами и семантическим поиском по смыслу документов.
- **Контекстный поиск** - Простой поиск релевантных фрагментов кода по ключевым словам (как в Continue.dev @codebase).
- **Embedding** - Вектор «смысла» текста. Используется в RAG, чтобы найти похожие куски кода по смыслу, а не по словам.
- **Temperature** - «Креативность». Для кода ставим 0.1–0.3, чтобы меньше фантазировала.
- **Ollama** - Локальный сервер LLM, управляет моделями (pull/run/push) через CLI + REST API. 

### Локальные vs Облачные модели: когда что выбрать

| Критерий | Локальные (Ollama) | Облачные (ChatGPT, Claude) |
|----------|--------------------|----------------------------|
| **Приватность** | ✅ Полная | ❌ Данные уходят в облако |
| **Стоимость** | ✅ Бесплатно после покупки железа | ❌ Постоянные расходы |
| **Скорость** | ⚡ Зависит от железа | ⚡ Зависит от интернета |
| **Качество** | ⚠️ Хуже на сложных задачах | ✅ Лучше качество |
| **Доступность** | ✅ Офлайн работа | ❌ Нужен интернет |
| **Настройка** | ⚠️ Требует времени | ✅ Готово из коробки |

**Выбирайте локальные модели, если:**
- Работаете с конфиденциальным кодом
- Нужна стабильная работа без интернета  
- Бюджет ограничен
- Требуется кастомизация под проект

**Выбирайте облачные модели, если:**
- Нужно максимальное качество ответов
- Работаете с очень сложными задачами
- Не хотите заниматься настройкой
- Железо не позволяет запустить локально

**Гибридный подход:**
- Локальные модели для рутинных задач (автодополнение, простые вопросы)
- Облачные для сложных задач (архитектурные решения, сложный рефакторинг)

#### Как выбрать модель (чек-лист)
1. **Универсальные и специализированные модели**
- Для генирации кода — используйте специализированные модели, например, codellama или deepseek-coder. Они лучше понимают синтаксис и контекст программирования.
- Для общения и общего назначения — подойдут универсальные модели, такие как llama3, mistral, phi3. Они хорошо справляются с разными задачами: объяснения, генерация текстов, ответы на вопросы.

2. **Язык**\
Некоторые модели лучше работают с английским, другие поддерживают много языков.

3. **Лицензия**\
Убедитесь, что лицензия модели подходит для ваших целей (open-source, коммерческое использование и т.д.).

4. **Размер модели**\
Большие модели точнее, но требуют больше ресурсов. Маленькие — быстрее, но могут ошибаться чаще.\
|RAM/VRAM| Размер модели |
| --- | --- |
|4–6 ГБ|mini модели ~4b|
|8–16 ГБ|8b - 14b|
|32 ГБ и выше|> 32b|

    Рекомендации:
    - `1.5B-7B` – маленькие быстрые модели для автодополнения
    - `13B-34B` – средние модели для сложных задач
    - `deepseek-coder`, `codellama` – специализированные модели для code review
<br>

5. **Квантизация моделей**\
  Квантизация — это сжатие модели для экономии памяти и ускорения работы за счёт небольшой потери качества.

    Форматы квантизации в Ollama:
    - `fp16` — полная точность (самое высокое качество, больше всего памяти)
    - `q8_0` — 8-битная квантизация (хороший баланс)
    - `q4_0` — 4-битная квантизация (быстро, экономично)
    - `q2_k` — агрессивное сжатие (для слабого железа)

    Примеры:
    ```sh
    # Полная модель ~26GB
    ollama pull llama3:70b

    # Квантизованная версия ~4.1GB  
    ollama pull llama3:70b-q2_k

    # Список доступных вариантов
    ollama show llama3 --versions
    ```

    Рекомендации:
    - Для серьёзной работы: `q8_0` или `fp16`
    - Для экспериментов: `q4_0` 
    - Для слабого железа: `q2_k`

6. **Оценка производительности моделей**\
Перед выбором модели важно протестировать её на ваших конкретных задачах:

    Метрики для оценки:
    - **Скорость генерации** — tokens/sec (измеряйте командой `ollama run model --verbose`)
    - **Качество кода** — компилируемость, логическая корректность
    - **Понимание контекста** — насколько точно модель следует инструкциям
    - **Потребление памяти** — мониторьте через `htop` или Activity Monitor

    Список доступных моделей можно посмотреть на сайте [Ollama](https://ollama.com/search).

**Совет:**
Тестируйте несколько моделей на своих задачах — это лучший способ выбрать подходящую!

### Установка Ollama
Ollama — это простой способ запускать LLM локально на вашем компьютере. Инструкцию для установки на другие ОС можно посмотреть на [официальном сайте](https://ollama.com/download).
1. Установка на macOS
```sh
brew install ollama
```
2. Запуск сервиса
```sh
ollama serve 
```
3. Загрузка модели
```sh
ollama pull qwen2.5-coder:1.5b-base
```

#### Интеграция с IDE
Интеграция ИИ-агентов в IDE ускоряет разработку за счёт глубокого понимания контекста проекта, автоматического исправления ошибок, рефакторинга кода и генерации решений в реальном времени — без утечки данных за пределы вашей среды.

##### Continue (VS Code / JetBrains extension)
1. **Установите плагин Continue для VS Code или JetBrains IDE.**
2. **Добавьте в `config.yaml` свои модели:**
```sh
➜  ~ cat ~/.continue/config.yaml 
name: Local Assistant
version: 1.0.0
schema: v1
models:
  - name: Qwen2.5-Coder 1.5B
    provider: ollama
    model: qwen2.5-coder:1.5b-base
    roles:
      - autocomplete
  - name: Gemma 3
    provider: ollama
    model: gemma3:12b
  - name: Сodellama
    provider: ollama
    model: codellama:13b
context:
  - provider: code
  - provider: docs
  - provider: terminal
```
- roles — специальные задачи для конкретной модели\
Параметр roles определяет, для каких задач используется эта модель (например, автодополнение кода).
Если у других моделей этот параметр не указан, они будут использоваться для обычных запросов (чата, генерации кода и т.д.).
- context — это источники данных для ассистента:
  + code — код текущего файла.
  + docs — документация проекта.
  + diff — изменения в коде (различия между версиями).
  + terminal — вывод терминала.
  + problems — список ошибок и предупреждений из редактора.
  + folder — содержимое текущей папки.
  + codebase — весь код проекта.
3. **После этого можно нажать на @ и добавить поставщика контекста:**
- files - ссылка на любой файл в рабочем пространстве.
- code - ссылка на конкретные функции или классы из вашего проекта.
- docs - ссылка на содержимое любого сайта с документацией.
- git diff - изменения в коде с момента последнего коммита.
- terminal - просмотрите последнюю команду, которую вы выполнили в терминале IDE, и ее вывод.
- problems - получить проблемы из текущего файла.
- folder - использует тот же механизм поиска, что и @Codebase, но только для одной папки.
- codebase - ссылка на наиболее релевантные фрагменты из вашей кодовой базы.

```sh
context:
  - provider: codebase
    name: "My Project"
    ignore:
      - node_modules
      - .git
      - dist
      - build
```

Каждый из этих поставщиков может быть использован для получения контекста, который затем передается в модель ИИ, чтобы дать более точный и релевантный ответ. Больше примеров можно посмотреть на официальном [сайте](https://docs.continue.dev/customize/custom-providers).


##### aichat (VS Code / JetBrains extension)
aichat — это удобный CLI-интерфейс для общения с LLM, который даёт больше возможностей по сравнению с обычной ollama-консолью:
1. **Установка aichat**
```sh
brew install aichat
```
для настройки конфига выбирайте openai-compatible
```
➜  ~ aichat
> No config file, create a new one? Yes
> API Provider (required): openai-compatible
> Provider Name (required): ollama
> API Base (required): http://localhost:11434/v1
> API Key (optional): 
> LLMs to include (required): gemma3:12b
✓ Saved the config file to '~/Library/Application Support/aichat/config.yaml'.

Welcome to aichat 0.29.0
Type ".help" for additional help.
```

Конфигурационный файл:
```sh
➜  ~ cat ~/Library/Application\ Support/aichat/config.yaml
# see https://github.com/sigoden/aichat/blob/main/config.example.yaml

clients:
- type: openai-compatible
  name: ollama
  api_base: http://localhost:11434/v1
  models:
    - name: gemma3:12b
    - name: codellama:13b
    - name: qwen2.5-coder:1.5b-base
```

2. **Пример работы:**
- Отправка запроса напрямую из терминала:
```sh
aichat "Объясни, как работает функция map в Ruby"
```

- Использование определённой модели:
```sh
aichat --model ollama:qwen2.5-coder:1.5b-base "Придумай SQL-запрос для выборки пользователей"
```

- Передача содержимого файла в чат:
```sh
aichat "Проанализируй этот код:" < main.rb

aichat -f main.rb "Найди баги в этом коде и предложи исправления"
```

- Использование ссылки в вопросе:
```sh
aichat -f https://github.com/rails/rails/blob/main/README.md "Кратко опиши, что делает этот проект"
```

### Быстрый старт с AnythingLLM
1. **Установка через Docker:**
```sh
docker run -d \
  -p 3001:3001 \
  --name anything-llm \
  -v ~/anythingllm:/app/server/storage \
  -e STORAGE_DIR="/app/server/storage" \
  mintplexlabs/anythingllm:latest
```

2. **Подключение к Ollama:**
   - Откройте `http://localhost:3001`
   - Settings → LLM Preference → Ollama
   - Base URL: `http://host.docker.internal:11434`
   - Model: выберите вашу модель

3. **Создание workspace:**
   - Создайте новый workspace для проекта
   - Загрузите документы (PDF, TXT, MD, код)
   - AnythingLLM автоматически создаст векторные эмбеддинги

### Контекстный поиск с Continue.dev
Continue.dev предоставляет умный поиск по кодовой базе через провайдер `@codebase`:
```yaml
contextProviders:
  - name: codebase
    params:
      nRetrieve: 25
      nFinal: 5
      useReranking: true
```

**Важно:** Это НЕ настоящий RAG, а контекстный поиск — Continue.dev находит релевантные фрагменты кода и добавляет их в контекст запроса, но без векторных эмбеддингов и семантического поиска.

**Использование:**
```sh
@codebase Как настроена аутентификация в этом проекте?
@docs Покажи примеры использования API
```

### Как работать эффективно:
1. **Борьба с галлюцинациями:**
- Жесткий контекст: "Отвечай ТОЛЬКО на основе предоставленного кода".
- Температура: Установите temperature=0.3 (меньше креатива → больше точности).
- Chain-of-Thought: Просите модель рассуждать шагами: "Сначала объясни, что делает функция, потом предложи фикс".

2. **Промптинг для разработки:**\
Будьте максимально конкретны, добавляйте контекст, если используете RAG — укажите, что нужно искать ответ в документации.
```sh
[Роль] Ты senior {язык} разработчик.  
[Задача] Напиши функцию, которая {описание}.  
[Ограничения] Не используй внешние библиотеки.  
[Пример] Вот аналогичный код: {пример}.  
```

3. **Если ИИ ошибается:**
- Проверяйте ответы:
    + Если ассистент с самого начала отвечает не по существу, не пытайтесь исправить ситуацию в этом диалоге — лучше сразу перезапустите чат и сформулируйте запрос заново.
LLM может придумывать несуществующие функции или факты. 
    + Ограничивайте область задачи и уточняйте промпт.

    ```sh
    "Почему ты считаешь, что это правильно?" → заставит модель перепроверить цепочку.
    ```
    ```sh
    "Покажи документацию из индекса RAG" → сверим с источниками.
    ```
- Используйте RAG для больших данных:\
Если нужно работать с большими объёмами информации (документация, база знаний), подключайте RAG — он подставит только релевантные куски в контекст.

- Следите за размером истории:\
Длинные переписки могут вытеснить важные детали. Если что-то важно — повторите это в новом запросе.

- Разделяйте задачи:\
Для сложных задач разбивайте общение на этапы, чтобы каждый шаг помещался в контекст.

- Понимайте ограничения:\
Модель не «помнит» ничего между сессиями, если не использовать внешние инструменты. Всё, что не в текущем окне — забывается.

### Решение частых проблем (Troubleshooting)

#### Проблемы с производительностью
**Медленная генерация:**
```sh
# Проверьте, не использует ли модель swap
htop  # или Activity Monitor на Mac

# Освободите память, остановив неиспользуемые модели
ollama stop model-name

# Используйте квантизованные версии моделей
ollama pull qwen2.5-coder:7b-q4_0  # вместо полной версии
```

**Нехватка памяти:**
- Используйте меньшие модели (1.5B-7B вместо 13B+)
- Закройте другие приложения
- Настройте `OLLAMA_MAX_LOADED_MODELS=1` в переменных среды

#### Проблемы с подключением
**Ollama не отвечает:**
```sh
# Проверьте статус сервиса
curl http://localhost:11434/api/tags

# Перезапустите Ollama
ollama serve

# Проверьте логи (на Mac)
tail -f ~/Library/Logs/ollama.log
```

**Continue.dev не видит модели:**
- Убедитесь, что Ollama запущен: `ollama list`
- Проверьте config.json: правильный ли `apiBase`
- Перезапустите VS Code

#### Проблемы с качеством ответов
**Модель "галлюцинирует":**
- Понизьте `temperature` до 0.1-0.3
- Используйте более конкретные промпты
- Добавляйте контекст через RAG

**Медленные ответы в Continue.dev:**
- Уменьшите `nRetrieve` в codebase provider
- Исключите большие файлы через `.continueignore`

#### Как устроена память в LLM и почему это важно
У больших языковых моделей (LLM) память — это не привычная нам оперативная память компьютера, а ограниченный «контекст» (context window), в который помещается только определённое количество токенов (слов и символов). Обычно это от 4 000 до 128 000 токенов, в зависимости от модели.

| Тип памяти | Где хранится | Что содержит | Управление пользователем | Доступность/особенности |
| --- | --- | --- | --- | --- |
| Контекстное окно | RAM inference-сервера | История текущего чата, токены | Нет | Обрезается по лимиту, теряется при сбросе |
| KV-кеш (вспомогательный) | RAM inference-сервера | Внутренние ключи/значения внимания | Нет | Только для ускорения генерации |
| Сессионная память | Backend/SaaS | Метаданные, настройки, ID | Нет | Не видна, подмешивается автоматически |
| Чатовая память | Backend/SaaS | История всех чатов | Частично (через UI) | Можно искать, иногда — экспортировать |
| Глобальная память | Backend/SaaS | Персональные факты, интересы | Частично (в настройках) | Можно очистить или отключить |
| Проектная/корпоративная | Backend/SaaS | Документы, базы знаний | Да (загрузка, удаление) | Ограничения по слотам и размеру |
| Внешняя RAG-память | Внешние сервисы/API | Векторные коллекции текстов | Да (API/настройки) | Подключается через API, не всегда доступна |
|  |  |  |  |  |

### Готовые промпты для разработки

#### Code Review
```
[РОЛЬ] Ты senior разработчик с 10+ лет опыта.
[ЗАДАЧА] Проведи code review этого кода:
{код}
[ФОКУС] Обрати внимание на:
- Безопасность и уязвимости
- Производительность
- Читаемость и поддерживаемость
- Соответствие best practices
[ФОРМАТ] Список конкретных замечаний с примерами исправлений.
```

#### Генерация тестов
```
[РОЛЬ] Ты QA engineer.
[ЗАДАЧА] Напиши unit тесты для этой функции:
{код функции}
[ТРЕБОВАНИЯ] 
- Покрой edge cases
- Используй {фреймворк тестирования}
- Добавь комментарии к сложным тестам
[ФОРМАТ] Готовый код тестов с импортами.
```

#### Рефакторинг
```
[РОЛЬ] Ты architect с экспертизой в {язык программирования}.
[ЗАДАЧА] Отрефактори этот код:
{код}
[ЦЕЛИ] 
- Улучши читаемость
- Следуй принципам SOLID
- Оптимизируй производительность
[ОГРАНИЧЕНИЯ] Не меняй публичный API.
```

#### Объяснение кода
```
[РОЛЬ] Ты технический ментор.
[ЗАДАЧА] Объясни этот код простыми словами:
{код}
[АУДИТОРИЯ] Junior разработчик.
[ФОРМАТ] 
1. Что делает код (общая идея)
2. Пошаговое объяснение
3. Ключевые концепции для изучения
```

#### Генерация документации
```
[РОЛЬ] Ты technical writer.
[ЗАДАЧА] Создай документацию для этого API:
{код API}
[ВКЛЮЧИ]
- Описание методов и параметров
- Примеры запросов/ответов
- Возможные ошибки
[ФОРМАТ] Markdown для README.md
```

### Заключение
Локальные LLM и ИИ-агенты — мощный инструмент для ускорения разработки. С Ollama и RAG вы можете быстро внедрить ИИ в свой рабочий процесс, сделать его удобным и безопасным.

ИИ-ассистент может значительно ускорить и упростить работу программиста:

- Генерация кода: Быстро пишет функции, классы, шаблоны и примеры на любом языке.
- Объяснение кода: Поясняет, как работает фрагмент кода, помогает разобраться в чужих проектах.
- Поиск ошибок и уязвимостей: Анализирует код, находит баги, предлагает исправления.
- Автоматизация рутинных задач: Генерирует тесты, документацию, миграции, SQL-запросы и др.
- Поиск информации: Быстро отвечает на вопросы по синтаксису, библиотекам, паттернам проектирования.
- Работа с документацией: Находит нужные фрагменты в больших документах или кодовой базе.
- Помощь с архитектурой: Предлагает варианты реализации, сравнивает подходы.
- Обучение и развитие: Помогает изучать новые технологии, объясняет сложные темы простыми словами.

### Полезные ссылки:
- [Ollama](https://ollama.com/) — официальный сайт
- [Ollama Models](https://ollama.com/search) — каталог доступных моделей
- [Continue.dev](https://continue.dev/) — ИИ-ассистент для VS Code/JetBrains
- [Continue.dev Docs](https://docs.continue.dev/) — документация по настройке
- [aichat](https://github.com/sigoden/aichat) — CLI-интерфейс для LLM
- [AnythingLLM](https://anythingllm.com/) — веб-интерфейс для RAG
- [Awesome Ollama](https://github.com/jmorganca/ollama/blob/main/docs/awesome.md) — коллекция инструментов
- [Как работает память в LLM](https://habr.com/ru/articles/928020/) — подробно о контексте и памяти